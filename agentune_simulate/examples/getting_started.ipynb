{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Agentune Simulate\n",
    "\n",
    "This notebook demonstrates how to use the Agentune Simulate library to create realistic customer-agent conversations using RAG (Retrieval-Augmented Generation) techniques.\n",
    "\n",
    "## What you'll learn:\n",
    "- Load conversation data from CSV files\n",
    "- Set up vector stores for RAG-based simulation\n",
    "- Run simulations with both in-memory and persistent storage\n",
    "- Analyze simulation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install langchain-chroma pandas\n",
    "!pip install agentune-simulate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import logging\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from agentune.simulate.models import Outcomes\n",
    "from agentune.simulate.rag import conversations_to_langchain_documents\n",
    "from agentune.simulate.simulation.session_builder import SimulationSessionBuilder\n",
    "\n",
    "# Import example utilities\n",
    "from utils import load_conversations_from_csv, extract_outcomes_from_conversations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup and API Key Configuration\n",
    "\n",
    "This example uses OpenAI models, but any LangChain-compatible LLM can be supported. Configure your API key for the model provider you choose:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set up OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"✓ API key configured\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configure basic logging to see simulation progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "\n",
    "# Fix asyncio event loop for Jupyter notebooks, to allow async code execution\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"✓ Logging configured\")\n",
    "print(\"✓ Asyncio event loop configured for Jupyter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Sample Data\n",
    "\n",
    "**Important**: For any data format or source, you must convert your data to `Conversation` objects for the Agentune Simulate to work.\n",
    "\n",
    "This example shows loading from CSV format:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load_conversations_from_csv is an example utility function that converts CSV data to Conversation objects\n",
    "# Example data is based on dhc2 dataset\n",
    "# You need to implement a similar function for your data format and schema\n",
    "conversations = load_conversations_from_csv(\"data/sample_conversations.csv\")\n",
    "\n",
    "print(f\"Loaded {len(conversations)} conversations\")\n",
    "print(f\"Sample conversation has {len(conversations[0].messages)} messages\")\n",
    "print(f\"First message: {conversations[0].messages[0].content[:100]}...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore the data structure\n",
    "df = pd.read_csv(\"data/sample_conversations.csv\")\n",
    "print(\"Dataset overview:\")\n",
    "print(f\"- Total messages: {len(df)}\")\n",
    "print(f\"- Unique conversations: {df['conversation_id'].nunique()}\")\n",
    "print(f\"- Message distribution: {df['sender'].value_counts().to_dict()}\")\n",
    "print(f\"- Outcome distribution: {df['outcome_name'].value_counts().to_dict()}\")\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Outcomes for Simulation\n",
    "\n",
    "Extract unique outcomes that our simulation will try to achieve:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract unique outcomes from conversations\n",
    "# Alternatively, you can define outcomes manually if you know them in advance\n",
    "unique_outcomes = extract_outcomes_from_conversations(conversations)\n",
    "outcomes = Outcomes(outcomes=tuple(unique_outcomes))\n",
    "\n",
    "print(f\"Found {len(unique_outcomes)} unique outcomes:\")\n",
    "for outcome in unique_outcomes:\n",
    "    print(f\"- {outcome.name}: {outcome.description}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You can also define outcomes manually if you know them in advance, instead of extracting them from existing conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Quick Simulation with InMemoryVectorStore\n",
    "\n",
    "Let's start with a simple in-memory vector store for quick demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup models - OpenAI models work well, other LangChain-compatible models can also be used\n",
    "# Note: gpt-4o has been tested and performs best for realistic conversations\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Use all conversations for vector store training (simplified approach)\n",
    "# Advanced: you could split data to reserve some conversations for validation\n",
    "training_conversations = conversations\n",
    "print(f\"Using {len(training_conversations)} conversations for vector store training\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create in-memory vector store\n",
    "documents = conversations_to_langchain_documents(training_conversations)\n",
    "vector_store = InMemoryVectorStore.from_documents(documents, embeddings)\n",
    "\n",
    "print(f\"✓ Created in-memory vector store with {len(documents)} documents\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build and run simulation session\n",
    "# SimulationSessionBuilder provides an opinionated, easy-to-use interface for creating simulation sessions\n",
    "# For more advanced configuration, see the documentation\n",
    "session = SimulationSessionBuilder(\n",
    "    default_chat_model=chat_model,\n",
    "    outcomes=outcomes,\n",
    "    vector_store=vector_store,\n",
    "    max_messages=10\n",
    ").build()\n",
    "\n",
    "# Run simulation - provide real conversations to use as starting points\n",
    "base_conversations = conversations[:5]  # Use first 5 conversations as starting points\n",
    "result = await session.run_simulation(real_conversations=base_conversations)    # 5 simulated conversations will be generated\n",
    "\n",
    "print(\"✓ Simulation completed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyze results using built-in methods\n",
    "print(result.generate_summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Persistent Storage with Chroma\n",
    "\n",
    "Chroma is a popular vector store for production use, allowing you to store vector data persistently and reuse it across sessions. Other LangChain-compatible vector stores can also be used.\n",
    "\n",
    "For production use, you'll want persistent vector storage. Here's how to use Chroma:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create persistent Chroma vector store\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "chroma_store = Chroma(\n",
    "    collection_name=\"conversation_examples\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "chroma_store.add_documents(documents)\n",
    "print(f\"✓ Added {len(documents)} documents to Chroma\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build session with Chroma vector store  \n",
    "chroma_session = SimulationSessionBuilder(\n",
    "    default_chat_model=chat_model,\n",
    "    outcomes=outcomes,\n",
    "    vector_store=chroma_store,\n",
    ").build()\n",
    "\n",
    "# Run simulation with Chroma\n",
    "chroma_result = await chroma_session.run_simulation(real_conversations=base_conversations)\n",
    "\n",
    "print(\"✓ Chroma simulation completed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare results and save\n",
    "print(\"=== CHROMA RESULTS ===\")\n",
    "print(chroma_result.generate_summary())\n",
    "\n",
    "# Save results to file using built-in method\n",
    "output_file = \"chroma_simulation_results.json\"\n",
    "chroma_result.save_to_file(output_file)\n",
    "print(f\"\\n✓ Results saved to {Path(output_file).absolute()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've seen the basics, you can:\n",
    "\n",
    "1. **Use your own data**: Convert your conversations to the expected format and load them using `load_conversations_from_csv()` or implement a custom loader\n",
    "2. **Scale up**: Run larger simulations with more conversations and longer interactions\n",
    "3. **Analysis**: Use the structured results for deeper analysis and visualization\n",
    "\n",
    "### Converting your data to the expected format:\n",
    "\n",
    "If you have tabular data, convert it to a DataFrame with these columns:\n",
    "- `conversation_id`: Unique identifier for each conversation\n",
    "- `sender`: Either \"customer\" or \"agent\"\n",
    "- `content`: The message content\n",
    "- `timestamp`: ISO format timestamp\n",
    "- `outcome_name`: Name of the conversation outcome\n",
    "- `outcome_description`: Description of the outcome\n",
    "\n",
    "Then use `load_conversations_from_csv()` to convert to `Conversation` objects.\n",
    "\n",
    "### Resources:\n",
    "- [Documentation](../README.md)\n",
    "- [Streamlit web interface](../streamlit/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
