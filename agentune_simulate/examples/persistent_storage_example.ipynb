{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent Storage and Production Features with Agentune Simulate\n",
    "\n",
    "  This notebook builds on the basic concepts from `getting_started.ipynb` to\n",
    "  demonstrate production-ready features of Agentune Simulate.\n",
    "\n",
    "  **Prerequisites**: Complete `getting_started.ipynb` first to understand the basics\n",
    "  of conversation simulation.\n",
    "\n",
    "  ## What you'll learn:\n",
    "  - Set up persistent vector storage with Chroma for production use\n",
    "  - Reuse vector stores across sessions to save time and resources\n",
    "  - Handle larger datasets efficiently with persistent storage\n",
    "  - Best practices for production deployments and scaling\n",
    "  - Advanced configuration options for real-world scenarios\n",
    "\n",
    "  ## Key Benefits of Persistent Storage:\n",
    "  - **Performance**: No need to rebuild vector stores each session\n",
    "  - **Scalability**: Handle thousands of conversations efficiently\n",
    "  - **Cost Efficiency**: Reduce embedding computation costs through reuse\n",
    "  - **Production Ready**: Suitable for deployment in production environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install langchain-chroma pandas\n",
    "!pip install agentune-simulate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from agentune.simulate.models import Outcomes\n",
    "from agentune.simulate.rag import conversations_to_langchain_documents\n",
    "from agentune.simulate.simulation.session_builder import SimulationSessionBuilder\n",
    "\n",
    "# Import example utilities\n",
    "from utils import setup_logging_and_asyncio, load_conversations_from_csv, extract_outcomes_from_conversations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup and API Key Configuration\n",
    "\n",
    "This example uses OpenAI models, but any LangChain-compatible LLM can be supported. Configure your API key for the model provider you choose:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set up OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"✓ API key configured\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configure logging and asyncio for Jupyter\n",
    "setup_logging_and_asyncio()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Sample Data\n",
    "\n",
    "**Important**: For any data format or source, you must convert your data to `Conversation` objects for the simulator to work.\n",
    "\n",
    "This example shows loading from CSV format:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load_conversations_from_csv is an example utility function that converts CSV data to Conversation objects\n",
    "# Example data is based on dhc2 dataset\n",
    "# You need to implement a similar function for your data format and schema\n",
    "conversations = load_conversations_from_csv(\"data/sample_conversations.csv\")\n",
    "\n",
    "print(f\"Loaded {len(conversations)} conversations\")\n",
    "print(f\"Sample conversation has {len(conversations[0].messages)} messages\")\n",
    "print(f\"First message: {conversations[0].messages[0].content[:100]}...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore the data structure\n",
    "df = pd.read_csv(\"data/sample_conversations.csv\")\n",
    "print(\"Dataset overview:\")\n",
    "print(f\"- Total messages: {len(df)}\")\n",
    "print(f\"- Unique conversations: {df['conversation_id'].nunique()}\")\n",
    "print(f\"- Message distribution: {df['sender'].value_counts().to_dict()}\")\n",
    "print(f\"- Outcome distribution: {df['outcome_name'].value_counts().to_dict()}\")\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Outcomes for Simulation\n",
    "\n",
    "Extract unique outcomes that our simulation will try to achieve:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract unique outcomes from conversations\n",
    "# Alternatively, you can define outcomes manually if you know them in advance\n",
    "unique_outcomes = extract_outcomes_from_conversations(conversations)\n",
    "outcomes = Outcomes(outcomes=tuple(unique_outcomes))\n",
    "\n",
    "print(f\"Found {len(unique_outcomes)} unique outcomes:\")\n",
    "for outcome in unique_outcomes:\n",
    "    print(f\"- {outcome.name}: {outcome.description}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You can also define outcomes manually if you know them in advance, instead of extracting them from existing conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models and Vector Store\n",
    "\n",
    "Chroma is a popular vector store for production use, allowing you to store vector data persistently and reuse it across sessions. Other LangChain-compatible vector stores can also be used.\n",
    "\n",
    "For production use, you'll want persistent vector storage. Here's how to use Chroma:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup models - OpenAI models work well, other LangChain-compatible models can also be used\n",
    "# Note: gpt-4o has been tested and performs best for realistic conversations\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Use all conversations for vector store training (simplified approach)\n",
    "# Advanced: you could split data to reserve some conversations for validation\n",
    "training_conversations = conversations\n",
    "print(f\"Using {len(training_conversations)} conversations for vector store training\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create persistent Chroma vector store\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "chroma_store = Chroma(\n",
    "    collection_name=\"conversation_examples\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "documents = conversations_to_langchain_documents(conversations)\n",
    "chroma_store.add_documents(documents)\n",
    "print(f\"✓ Added {len(documents)} documents to Chroma\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Simulation"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build session with Chroma vector store  \n",
    "chroma_session = SimulationSessionBuilder(\n",
    "    default_chat_model=chat_model,\n",
    "    outcomes=outcomes,\n",
    "    vector_store=chroma_store,\n",
    ").build()\n",
    "\n",
    "# Run simulation with Chroma\n",
    "base_conversations = conversations[:5]\n",
    "chroma_result = await chroma_session.run_simulation(real_conversations=base_conversations)\n",
    "\n",
    "print(\"✓ Chroma simulation completed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare results and save\n",
    "print(\"=== CHROMA RESULTS ===\")\n",
    "print(chroma_result.generate_summary())\n",
    "\n",
    "# Save results to file using built-in method\n",
    "output_file = \"chroma_simulation_results.json\"\n",
    "chroma_result.save_to_file(output_file)\n",
    "print(f\"\\n✓ Results saved to {Path(output_file).absolute()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've completed a basic simulation:\n",
    "\n",
    "1. **Use your own data**: Load your own conversations as a list of `Conversation` objects and use them to set up the simulation.\n",
    "2. **Explore advanced features**: Check out the full documentation for more options caching of LLM responses, LLM failures handling, and more.\n",
    "\n",
    "### Resources:\n",
    "- [Full documentation](https://github.com/SparkBeyond/agentune/blob/main/agentune_simulate/README.md)\n",
    "- [Complete examples](https://github.com/SparkBeyond/agentune/tree/main/agentune_simulate/examples)\n",
    "- [Streamlit web interface](https://github.com/SparkBeyond/agentune/blob/main/agentune_simulate/streamlit/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
